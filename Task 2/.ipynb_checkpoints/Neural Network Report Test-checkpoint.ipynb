{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We are going to use sklearn to load the dataset, this will be the only time we will use it during this report. Only to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from abc import ABC, abstractmethod\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "10000\n",
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_new = to_categorical(y)\n",
    "print(y_new[2])\n",
    "\n",
    "split = 60000\n",
    "split_test = X.shape[0] - split\n",
    "\n",
    "print(split_test)\n",
    "\n",
    "X_train = X[:split]\n",
    "y_train = y_new[:split]\n",
    "\n",
    "X_test = X[split:]\n",
    "y_test = y_new[split:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Layer class\n",
    "\n",
    "We are going to make a Layer class that will provide two methods to be implemented by subclasses. The method is create_layer(self, inputs, seed) and forward(self, inputs) to provide the functionality of backpropagation and forward propagation of the layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \n",
    "    \"\"\" A layer of neurons in a neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, neurons: int):\n",
    "        self.neurons = neurons\n",
    "        self.weights = 0\n",
    "        self.is_initialized = True\n",
    "        self.gradients_param = []\n",
    "        self.operations = []\n",
    "        self.seed = 1\n",
    "    @abstractmethod \n",
    "    def create_layer(self, inputs, seed):\n",
    "        \"\"\" Creates layer \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calculate layer output using forward propagation for given input. \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod \n",
    "    def backward(self, ouputs):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the DenseLayer class\n",
    "\n",
    "We will make the DenseLayer that will implement the Layer class to provide backpropagation and forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-7ac8f795da2d>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7ac8f795da2d>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    \"\"\" Calculate layer output using forward propagation for given input. \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \n",
    "    def __init__(self, neurons: int, activation):\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def create_layer(self, inputs, seed):\n",
    "        \n",
    "        if isinstance(seed, int):\n",
    "            self.seed = np.random.seed(seed)\n",
    "        self.params = []\n",
    "\n",
    "        \n",
    "        #Add weights\n",
    "        self.params.append(np.random.randn(inputs.shape[1], self.neurons))\n",
    "\n",
    "        #bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "        \n",
    "        self\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "        \n",
    "        def forward(self, inputs):\n",
    "        \"\"\" Calculate layer output using forward propagation for given input. \"\"\"\n",
    "            pass\n",
    "        \n",
    "        def add_inputs(x, y, activation_func):\n",
    "            \n",
    "            output = x + y\n",
    "            return activation_func(output)\n",
    "\n",
    "        def add_inputs_backward(x, y, activation_func):\n",
    "            # Calculate \"forward pass\"\n",
    "            output = x + y\n",
    "            dsda = deriv(activation_func, output)\n",
    "            dadx, dady = 1, 1\n",
    "            \n",
    "            return dsda * dadx, dsda * dady\n",
    "        \n",
    "        def deriv(func, inputs, delta):\n",
    "            return (func(inputs + delta) - (func(inputs - delta)) / (2 * delta)\n",
    "        \n",
    "\n",
    "        def matmul_forward(X, W, activation_func):\n",
    "            \"\"\"Calculates the forward pass of a matrix multiplication\"\"\"\n",
    "            # matrix multiplication        \n",
    "            N = np.dot(X, W)\n",
    "            \n",
    "            # applying activation function \n",
    "            S = activation_func(N)\n",
    "            \n",
    "            # sum all the elements\n",
    "            L = np.sum(S)\n",
    "                    \n",
    "            return L\n",
    "\n",
    "        def matmul_backward_first(X, W, activation_func):\n",
    "            \"\"\"Calculates the backward pass of a matrix multiplication respect X\"\"\"  \n",
    "            #matrix multiplication of train and weights\n",
    "            N = np.dot(X, W)\n",
    "            \n",
    "            #applying activation function\n",
    "            S = sigma(N)\n",
    "                    \n",
    "            #backward calculation\n",
    "            \n",
    "            #dLdS - just 1s\n",
    "            dLdN = np.ones_like(S)\n",
    "            dSdN = deriv(sigma, N)\n",
    "            \n",
    "            dLdN = dLdS * dSdN\n",
    "                    \n",
    "            dNdX = np.transpose(W, (1, 0))\n",
    "             \n",
    "            dLdX = np.dot(dSdN, dNdX)\n",
    "            # multiply them together\n",
    "            return dLdX\n",
    "\n",
    "        def backward(self, ouputs):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Loss class\n",
    "\n",
    "This class will provide the loss functions that will be applied after the activation function has been applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def __init__(self, y_pred, y_true, type_m):\n",
    "        \n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        if type_m == 'msr':\n",
    "            self.mean_squared_error()\n",
    "        elif type_m == 'cat_crossentropy':\n",
    "            self.categorical_crossentropy()\n",
    "        else:\n",
    "            raise ValueError('Invalid loss function')\n",
    "            \n",
    "    def mean_squared_error():\n",
    "        return np.mean((self.y_pred - self.y_true) ** 2, axis = 1)\n",
    "    \n",
    "\n",
    "    def mean_squared_error_d():\n",
    "        return np.expands_dims((2/self.y_pred.shape[1]) * (self.y_pred - self.y_true), 1)\n",
    "    \n",
    "    def categorical_crossentropy():\n",
    "        return -np.log(np.sum(self.y_true * self.y_pred, axis=1) + EPS)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Activation class.\n",
    "\n",
    "This class will provide the activations functions that will be applied to the input and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \n",
    "\n",
    "    \"\"\"Class of an operation in a neural network such as forward and backward\"\"\"\n",
    "    def __init__(self, type_func):\n",
    "        if type_func == 'sigmoid':\n",
    "            self.act_func = self.sigmoid\n",
    "            self.act_func_d = self.sigmoid_d\n",
    "        elif type_func == 'relu':\n",
    "            self.act_func = self.relu\n",
    "            self.act_func_d = self.relu_d\n",
    "        elif type_func == 'tanh':\n",
    "            self.act_func = self.tanh\n",
    "            self.act_func_d = self.tanh_d\n",
    "        elif type_func == 'softmax':\n",
    "            self.act_func = self.softmax\n",
    "            self.act_func_d = self.softmax_d\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function.')\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.last_input = inputs\n",
    "        return self.act_func(inputs)\n",
    "\n",
    "        \n",
    "    def backward(self, output):\n",
    "        \"\"\"Returns the derivative of the activation function\"\"\"\n",
    "        return output * self.act_func_d(self.last_input)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "            \n",
    "        #if isDeriv: # When backpropagating\n",
    "            #return (np.exp(-z)) / ((np.exp(-z) + 1) **2)\n",
    "\n",
    "        # Forward feed\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "  \n",
    "    def sigmoid_d(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s*(1-s)\n",
    "\n",
    "   \n",
    "    # Relu function can also be applied to input/hidden layers\n",
    "    def relu(self, z):\n",
    "        \n",
    "        r = np.maximum(0, z)\n",
    "        return r\n",
    "    # Gradient of the Relu function \n",
    "    def relu_d(self, z):\n",
    "      \n",
    "        dz = np.zeros(z.shape)\n",
    "        dz[z >= 0] = 1\n",
    "        return dz\n",
    "# Softmax function to be applied to the output layer \n",
    "    def softmax(self, z, isDeriv):\n",
    "        expo = np.exp(z - z.max())\n",
    "        \n",
    "        if isDeriv: # When backpropagating\n",
    "            return expo / np.sum(expo, axis = 0) * (1 - expo / np.sum(expo, axis = 0))\n",
    "        \n",
    "        # Forward feed\n",
    "        return expo / np.sum(expo, axis = 0)    \n",
    "\n",
    "    def softmax_d(self, x):\n",
    "        s = self.softmax(x)\n",
    "        ds = np.stack([np.diag(s[i, :]) for i in range(s.shape[0])])\n",
    "  #Tanh function to be applied to the input/output layer  \n",
    "    def tanh(self, z):\n",
    "        \"\"\"Returns the tanh of x \"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "  #Gradient of tanh function \n",
    "    def tanh_d(self, z):\n",
    "        \"\"\"Returns the gradiend of tanh(x) function \"\"\"\n",
    "        e = np.exp(2*z)\n",
    "        return (e - 1)/(e + 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Optimizer class\n",
    "\n",
    "The Optimizer class will be implemented by subclasses to provide optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    \n",
    "    def __init__(self, weights, alpha=0.001, ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "         \n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, params, gradient):\n",
    "        \"\"\"Updates the change of the weights to minimise the error\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-a699c15dc7f6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-a699c15dc7f6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Adam(Optimizer):    \n",
    "    \n",
    "    def __init__(self, X, y, learning_rate=0.01, momentum = 0.0):\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \n",
    "    def __init__(self, X, y, learning_rate =0.01, momentum = 0.0):\n",
    "        super().__init__(learning_rate=learning_rate, momentum= momentum)\n",
    "        self.m = 0.2\n",
    "        self.b = 0.2\n",
    "        N = len(X)\n",
    "    def update(self, params, gradient, has_delta=False):\n",
    "        if not has_delta:\n",
    "                \n",
    "                self.delta = np.zeros_like(params)\n",
    "\n",
    "        self.delta = self.momentum\n",
    "        \n",
    "    def step(self):\n",
    "        #pass\n",
    "        #if self.first: #if  \n",
    "        for it in range(iterations):\n",
    "            cost = 0.0\n",
    "            for i in range(m):\n",
    "                random_int = np.random.randint(0, m)\n",
    "                X_i = X[random_int, :].reshape(1, X.shape[1])\n",
    "                y_i = y[random_int].reshape(1, 1)\n",
    "                pred = np.dot(Xi, theta)\n",
    "                self.update()\n",
    "                \n",
    "    def update(self, params, gradient, loss):\n",
    "        m -= self.learning_rate * (-2 * self.X.dot(f).sum() / N)\n",
    "        b -= self.learning_rate * (-2 * f.loss)\n",
    "        \n",
    "    def stocashtic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    \n",
    "    \n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0,m)\n",
    "            Xi = X[rand_ind,:].reshape(1,X.shape[1])\n",
    "            yi = y[rand_ind].reshape(1,1)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "\n",
    "            theta = theta -(1/m)*learning_rate*( Xi.T.dot((prediction - yi)))\n",
    "            cost += cal_cost(theta,Xi,yi)\n",
    "        cost_history[it]  = \n",
    "        \n",
    "    return theta, cost_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
